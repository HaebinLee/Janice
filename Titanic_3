#3 타이타닉 생존자 예측하기 - modeling, validation, testing

<Which classifier to use?>
1. KNN
주변의 이웃을 보고 다수로 판단.
ex. k=5인 경우 주변의 3명이 죽었으면 죽었다고 예측
    k에 따라 달라질 수 있다!!
    k를 변화시켜가며 정확도 살펴보기
  
2. Decision tree
Feature를 이용하여 트리구축

3. Random forest 
여러개의 조금 작은 디시전 트리로 이루어짐
ex. 9개의 feature가 있으니 세개 씩 세개의 트리를 구축한다고 해보자.
    각 트리에서의 결과를 살펴보고 두개의 트리가 죽었다는 결과를 내면 죽었다고 결론!

4. Naive Bayes
확률사용
ex. Feature가 Age Sex가 있다고 하면 P(survived|old,man)을 구함. 공식은 사진참고

5. SVM
죽은 사람과 산 사람 사이에 decision boundary를 구축하고 그것을 기준으로 판단 


<How do you validate your model?>
Testing전에 Train 데이터로 반드시 해야하는 Validation

891개의 데이터를 800개 train, 91개 vali : 정확도 90%
                91개 vali, 800개 train : 정확도 60%
=> 나누는 데이터 위치에 따라 달라지는 방법, 보완필요

1. K-fold cross validation
ex. k=10 
    train데이터를 10개로 등분
    validation set을 바꿔가며 10번 수행
    final accuracy : 10개의 정확도의 평균
    
---------------------------------------------------------------------------------------------
1. Importing Classifier Modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
import pandas as pd
import numpy as np


2. 데이터 가져오기
train = pd.read_csv('C:/Users/uos/Desktop/online_study/Kaggle_Titanic/train_new.csv')
test = pd.read_csv('C:/Users/uos/Desktop/online_study/Kaggle_Titanic/test_new.csv')
train_data = pd.read_csv('C:/Users/uos/Desktop/online_study/Kaggle_Titanic/train_data.csv')
target = pd.read_csv('C:/Users/uos/Desktop/online_study/Kaggle_Titanic/target.csv', header=None)
target = np.ravel(target) #기계가 원하는 1d array 형식으로


3. K-fold cross validation (K=10)
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
k_fold = KFold(n_splits=10, shuffle=True, random_state=0)

3-1. kNN (k=13)
clf = KNeighborsClassifier(n_neighbors = 13)
scoring = 'accuracy'
score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)
print(score) #10개의 CV스코어
round(np.mean(score)*100, 2) #kNN Score 82.6!

3-2. Decision tree
clf = DecisionTreeClassifier()
scoring = 'accuracy'
score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)
print(score)
round(np.mean(score)*100, 2) #decision tree Score 79.69!

3-3. Randon Forest
clf = RandomForestClassifier(n_estimators=13) #나무 13개
scoring = 'accuracy'
score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)
print(score)
round(np.mean(score)*100, 2) #"Random" Forest Score 약 81.03!

3-4. Naive Bayes
clf = GaussianNB()
scoring = 'accuracy'
score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)
print(score)
round(np.mean(score)*100, 2) #Naive Bayes Score 78.78!

3-5. SVM
clf = SVC()
scoring = 'accuracy'
score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)
print(score)
round(np.mean(score)*100,2) #SVM Score 83.5!

=> SVM점수가 가장 높다!!!


4. Testing
clf = SVC() #SVM Classifier
clf.fit(train_data, target) #데이터 train시키기

test_data = test.drop("PassengerId", axis=1).copy() #필요없는 거 드랍시킴
prediction = clf.predict(test_data)

4-1. 캐글 제출용으로 저장
submission = pd.DataFrame({
        "PassengerId": test["PassengerId"],
        "Survived": prediction
    })
submission.to_csv('C:/Users/uos/Desktop/online_study/Kaggle_Titanic/Submission.csv', index=False)

4-2. 캐글에 제출
https://www.kaggle.com/c/titanic/submit
=> 3425등 78.947%
