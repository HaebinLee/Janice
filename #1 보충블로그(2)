https://m.blog.naver.com/weplayicecream/221454185017

나의이웃 코코님을 위한 robots.txt와 데이터 스크랩(Scrap), 크롤링(Crawling) 하는 방법! 파이썬으로 수집할꺼예요! 
또 뭐가 필요하세요? 수집시 주의사항까지! 


데이터 스크랩, 크롤링 이란? 
크롤링(Crawling)이란 사전적으로 기어다니는 것을 뜻하는데, 전산쪽에서는 Web상을 돌아다니면서 정보를 수집하는 행위를 뜻한다. 
크롤링, 스크래핑(Scraping) 또는 데이터 긁기등 다양한 단어로 불린다.

크롤링의 대상은 웹 상의 자원들이다. 정적인 문서가 될 수도 있고, API와 같은 서비스가 될 수도 있다. 
다수의 데이터를 수집하고, 여기서 필요한 정보만 추출해서 처리하는 것을 우리는 크롤링이라고 부를뿐이다.   
예를 들어 구글과 같은 검색엔진에서는 웹 사이트의 정적인 데이터를 긁어다가 필요한 정보를 추출해서 검색 인덱스를 생성하고, 
가격 정보 비교 사이트는 상품과 가격정보등을 긁어다가 서로 다른 쇼핑몰의 가격을 알려주기도 한다. 
크롤러는 사실 그렇게 엄청난 것이 아니며, 필요에 따라 누구든지 제작할 수 있다.  

크롤링 매커니즘은 대략적으로 아래의 단계를 통해서 진행된다. 
(1) 대상 선정 -> (2) 데이터 로드 -> (3) 데이터 분석 -> (4) 데이터 수집

(1) 크롤링 대상 선정 (API 또는 웹 문서)
웹 상의 데이터는 고유한 ID를 가진다. URI라고 부르며, 이는 우리가 잘 하는 웹 사이트 주소인 URL과 URN이 있다.
간단하게 과일에 대한 네이버 검색 결과를 크롤링하려면 
"https://search.naver.com/search.naver?query=사과", "https://search.naver.com/search.naver?query=배"의 URL을 선정하는 과정이다.

(2) 데이터 로드 
데이터 로드는 웹 사이트를 켜는것과 같다. 
만약 API라면 XML, JSON 문서가 될 것이고, 웹 페이지라면 HTML 문서를 다운로드 받는 것이다.

(3) 데이터 분석
로드된 데이터에서 필요한 부분을 뽑아내는 것을 뜻한다. 당연하게도 웹 사이트상에는 내가 필요로하지 않는 부분이 많다.
어떠한 부분을 수집할지, 어떤 부분을 수집하지 않을지 선정하는 과정이다.

(4) 수집
데이터 분석 과정을 통해서 수집할 내용을 선정했다면, 이를 추출하여 파일 또는 데이터를 메모리상에 저장하는 과정이다.


!!데이터 수집에서 지켜야할 점도 많은것은 아시죠? 일단 저작권! 이런 법적문제는 알아서 지켜주셔요!!
일단 스크랩을 하기전에 확인해야할 파일이 있으니 그것은 바로! #robots.txt 의 파일입니다.

웹상에서는 여러 bot이 돌아다니며 웹페이지를 모은다. 
크롤러(스크랩퍼)가 어떻게 동작을 해야하는지 규제를 하고 있는게 robots.txt의 파일이예요.  
대부분 호스트주소 뒤에 /robots.txt 를 입력하면 해당 호스트에서 스크랩의 규제를 어떤방식으로 하고 있는지 설명이 되어 있습니다. 

# robots.txt example
Crawl-delay: 30
Request-rate: 1/30
매번 요청을 보내는 사이에 초는 30초를 기다리고, 30초마다 하나의 페이지만 요청하라는 것을 의미합니다. 

이처럼 규칙이 있는것을 꼭 인지하시고 페이지를 가져오셔야합니다. 그냥 긁었다가는 아이피가 차단될 가능성이 있습니다. 




예시!
크롬에서 위페이지를 열고 오른쪽 마우스를 누르면 Inspect 버튼이 있습니다. 누르면 아래와 같이 창이 뜨는데요. 
이 창에서 이제 어느 부분을 가져올지 확인이 가능합니다. 아래처럼 보이죠? 
여기서 아래 화살표 버튼을 누르고 가져오고 싶은 부분위에 올리면 오른쪽 HTML 코드에서 해당부분의 데이터가 선택이 됩니다. 

자 이제 그럼 다음 부분의 HTML 코드를 가져와서 파싱을 하면 끝이겠죠?

이제 코딩을 해봅시다. 준비물은 아래와 같아요.
* Python 3.x
* Beautiful Soup
* Requests


Beautiful Soup은 HTML, XML 의 데이터를 가져올때 사용하느 파이썬 라이브러리입니다. 
Requests는 GET, POST와 같이 HTTP 요청을 할때 사용하는 라이브러리입니다.
pip install BeautifulSoup4
pip install requests

일단 부동산 페이지를 requests를 통해 요청하고 값을 가져옵니다. 
r은 HTTP 호출을한 결과를 담고 있습니다. (응답을 제대로 받았는지 status_code 등을 확인이 가능합니다. r.status_code)
from bs4 import BeautifulSoup
import requests

url = "https://land.naver.com/"

r = requests.get(url)
soup = BeautifulSoup(r.content, 'html.parser')

다음과 같이 요청한 데이터를 r에 받아오고, r.content로 HTML의 코드를 BeautifulSoup의 html.parser를 통해 파싱하면
soup의 object<class 'bs4.BeautifulSoup'>를 얻을 수 있습니다. 
이렇게 얻은 HTML의 내용을 간단하게 find, find_all을 통해서 html tag를 찾으면 됩니다. html tag는 <a>, <div>, <ul> 뭐 이런 식입니다.

news = soup.find('div', attrs={'class': 'news NE=a:tdn'})
news_list = news.find('ul').find_all('li')

for n in news_list:
    print (n.find('a').attrs)
    print (n.find('a').attrs['href'])
    print (n.find('a').attrs['title'])

find()를 통해서 div의 클래스가 news NE=a:tdn을 가져오고, 그 안에 ul, li를 차례대로 가져왔습니다. 
이때 li를 가져올때는 find_all을 사용했는데, 그 이유는 ul안에 여러개의 li가 있기 때문입니다.  
이제 받아온 news_lists를 하나씩 for-loop으로 출력하면 끝입니다.


https://gist.github.com/restato/b9c77d30fca93e9b56d548ad22282cdb
